{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2_Z1knJa3Hqs"
   },
   "outputs": [],
   "source": [
    "# Auth for GDrive\n",
    "from google.colab import drive\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "from googleapiclient.discovery import build\n",
    "drive_service = build('drive', 'v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "KmiCHlF9cCyA",
    "outputId": "aab0ad9f-ccd7-4c28-e918-1d610ccd571b"
   },
   "outputs": [],
   "source": [
    "# Install Tensorboard\n",
    "!pip install -q tensorboardcolab\n",
    "from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "colab_type": "code",
    "id": "f3lLQdtGcLn2",
    "outputId": "8b805bc8-7c3d-46b6-807c-ff3f021ce08a"
   },
   "outputs": [],
   "source": [
    "# Download files from GDrive\n",
    "file_ids = {\n",
    "    'cb6133filtered':'1yYe9xfC9g8lYEsmDwC7_fP7l3CWQBKNp',\n",
    "    'cb6133':'1SS6hbyAXJV3oBMSrDxnaHa_dSV7Ijq7M',\n",
    "    'cb513':'1liYeocK2OQTn0mnBgNzl1dsTANHpP-pp'\n",
    "}\n",
    "\n",
    "import numpy as np\n",
    "import io\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "\n",
    "cb_data = {}\n",
    "\n",
    "for file_name, file_id in file_ids.items():\n",
    "    request = drive_service.files().get_media(fileId=file_id) \n",
    "    downloaded = io.BytesIO()\n",
    "    downloader = MediaIoBaseDownload(downloaded, request)\n",
    "    done = False\n",
    "\n",
    "    while done is False:\n",
    "        status, done = downloader.next_chunk()\n",
    "        print(\"Download %d%%.\" % int(status.progress() * 100))\n",
    "        \n",
    "    downloaded.seek(0)   \n",
    "    cb_data[file_name] = np.load(downloaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "6Dx3S3l7hTfI",
    "outputId": "b585876a-4735-4625-f98f-412aff062946"
   },
   "outputs": [],
   "source": [
    "# for uploading meta-features\n",
    "from google.colab import files\n",
    "data = files.upload()\n",
    "for fn in data.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(data[fn])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "lZZcczzei9Of",
    "outputId": "c0ad590d-84d9-4217-c57a-23e073cc8aa5"
   },
   "outputs": [],
   "source": [
    "cb513 = cb_data['cb513']\n",
    "cb6133 = cb_data['cb6133']\n",
    "cb6133filtered = cb_data['cb6133filtered']\n",
    "\n",
    "print(cb513.shape)\n",
    "print(cb6133.shape)\n",
    "print(cb6133filtered.shape)\n",
    "\n",
    "residue_list = ['A', 'C', 'E', 'D', 'G', 'F', 'I', 'H', 'K', 'M', 'L', 'N', 'Q', 'P', 'S', 'R', 'T', 'W', 'V', 'Y', 'X','NoSeq']\n",
    "q8_list = ['L', 'B', 'E', 'G', 'I', 'H', 'S', 'T','NoSeq']\n",
    "\n",
    "r = 700 # protein residues padded to 700\n",
    "f = 57 # number of features for each residue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "7wjgigzi6S2k",
    "outputId": "e997d4d6-3edd-4cc2-ab8a-25eaf1e49392"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model, Input, Sequential\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Concatenate, BatchNormalization\n",
    "from keras.layers import Bidirectional, Activation, Dropout, CuDNNGRU, CuDNNLSTM, Conv1D, GRU\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l1, l2\n",
    "import tensorflow as tf\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-ZnKXhP6dst"
   },
   "outputs": [],
   "source": [
    "def get_data(i, arr):\n",
    "    seq, q8, profiles = '', '', []\n",
    "    \n",
    "    for j in range(r):\n",
    "        jf = j*f\n",
    "        residue_onehot = arr[i,jf+0:jf+22] # residue one-hot encoded = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "        residue_q8_onehot = arr[i,jf+22:jf+31] # testing fields [22,31) q8 one-hot encoded = [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
    "        nc_terminals = arr[i,jf+31:jf+33] # nc_terminals start and end of sequence = [0. 0.]\n",
    "        sa = arr[i,jf+33:jf+35] # do not use these fields at all sa = [0. 0.]\n",
    "        profile = arr[i,jf+35:jf+57] # profile features\n",
    "        residue = residue_list[np.argmax(residue_onehot)] # decode residue symbol\n",
    "        residue_q8 = q8_list[np.argmax(residue_q8_onehot)] # decode q8 secondary structure symbol\n",
    "        \n",
    "        if residue == 'NoSeq': # terminating sequence symbol\n",
    "            break\n",
    "        seq += residue # concat residues into amino acid sequence\n",
    "        q8 += residue_q8 # concat secondary structure into secondary structure sequence\n",
    "        profiles.append(profile)\n",
    "\n",
    "    if len(seq) == len(q8): # verify amino acid sequence and secondary structure sequence lengths match\n",
    "        header = 'id,len,input,expected' # data header\n",
    "        line = str(i+1) + ',' + str(len(seq)) + ',' + seq + ',' + q8 # data row\n",
    "        return (str(i+1), str(len(seq)), seq, np.array(profiles), q8)\n",
    "        #print('\\n' + header + '\\n' + line) # example printout of header and protein data\n",
    "        \n",
    "    else:\n",
    "        print('length mismatch', str(len(seq)), str(len(q8)))\n",
    "\n",
    "# The custom accuracy metric used for this task\n",
    "def accuracy(y_true, y_pred):\n",
    "    y = tf.argmax(y_true, axis =- 1)\n",
    "    y_ = tf.argmax(y_pred, axis =- 1)\n",
    "    mask = tf.greater(y, 0)\n",
    "    return K.cast(K.equal(tf.boolean_mask(y, mask), tf.boolean_mask(y_, mask)), K.floatx())\n",
    "\n",
    "# eager execution accuracy\n",
    "def ex_accuracy(y_true, y_pred):\n",
    "    y = np.argmax(y_true, axis =- 1)\n",
    "    y_ = np.argmax(y_pred, axis =- 1)\n",
    "    mask = np.greater(y, 0)\n",
    "    arr = np.equal(y[mask], y_[mask])\n",
    "    return sum(arr) / float(len(arr))\n",
    "\n",
    "# Maps the sequence to a one-hot encoding\n",
    "def onehot_to_seq(oh_seq, index):\n",
    "    s = ''\n",
    "    for o in oh_seq:\n",
    "        i = np.argmax(o)\n",
    "        if i != 0:\n",
    "            s += index[i]\n",
    "        else:\n",
    "            break\n",
    "    return s\n",
    "\n",
    "def seq2onehot(seq, n):\n",
    "    out = np.zeros((len(seq), maxlen_seq, n))\n",
    "    for i in range(len(seq)):\n",
    "        for j in range(maxlen_seq):\n",
    "            out[i, j, seq[i, j]] = 1\n",
    "    return out\n",
    "\n",
    "# prints the results\n",
    "def print_results(x, y_, revsere_decoder_index):\n",
    "    # print(\"input     : \" + str(x))\n",
    "    # print(\"prediction: \" + str(onehot_to_seq(y_, revsere_decoder_index).upper()))\n",
    "    print(str(onehot_to_seq(y_, revsere_decoder_index).upper()))\n",
    "\n",
    "# Computes and returns the n-grams of a particualr sequence, defaults to trigrams\n",
    "def seq2ngrams(seqs, n = 1):\n",
    "    return np.array([[seq[i : i + n] for i in range(len(seq))] for seq in seqs])\n",
    "\n",
    "training_idx = range(5600)\n",
    "test_idx = range(5605,5877)\n",
    "validation_idx = range(5877,6133)\n",
    "\n",
    "training_idx2 = range(5534)\n",
    "test_idx2 = range(514)\n",
    "\n",
    "training_data = [get_data(i, cb6133) for i in training_idx]\n",
    "test_data = [get_data(i, cb6133) for i in test_idx]\n",
    "# training_data = [get_data(i, cb6133filtered) for i in training_idx2]\n",
    "# test_data = [get_data(i, cb513) for i in test_idx2]\n",
    "# validation_data = [get_data(i, cb6133) for i in validation_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xiCDtVyL7ILh"
   },
   "outputs": [],
   "source": [
    "training_data_df = pd.DataFrame(training_data)\n",
    "test_data_df = pd.DataFrame(test_data)\n",
    "validation_data_df = pd.DataFrame(test_data)\n",
    "\n",
    "columns = [\"id\", \"len\", \"input\", \"profiles\", \"expected\"]\n",
    "           \n",
    "training_data_df.columns = columns\n",
    "test_data_df.columns = columns\n",
    "validation_data_df.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AC9992Jz807Z",
    "outputId": "837e8eed-8d31-4584-b419-155a2f0703e1"
   },
   "outputs": [],
   "source": [
    "train_df = training_data_df\n",
    "test_df = test_data_df\n",
    "val_df = validation_data_df\n",
    "len(train_df), len(test_df), len(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "nFLncvJzkknq",
    "outputId": "e749daa4-59e4-4172-ee06-a3ea7fc79453"
   },
   "outputs": [],
   "source": [
    "maxlen_seq = 700\n",
    "\n",
    "# Load train inputs\n",
    "train_input_seqs, train_target_seqs = train_df[['input', 'expected']][train_df.len.astype(int) <= maxlen_seq].values.T\n",
    "train_input_grams = seq2ngrams(train_input_seqs)\n",
    "# Load test inputs\n",
    "test_input_seqs, test_target_seqs = test_df[['input', 'expected']][(test_df.len.astype(int) <= maxlen_seq)].values.T\n",
    "test_input_grams = seq2ngrams(test_input_seqs)\n",
    "# Load val inputs\n",
    "val_input_seqs, val_target_seqs = val_df[['input', 'expected']][(val_df.len.astype(int) <= maxlen_seq)].values.T\n",
    "val_input_grams = seq2ngrams(val_input_seqs)\n",
    "\n",
    "# Initializing and defining the tokenizer encoders and decoders based on the train set\n",
    "tokenizer_encoder = Tokenizer()\n",
    "tokenizer_encoder.fit_on_texts(train_input_grams)\n",
    "tokenizer_decoder = Tokenizer(char_level = True)\n",
    "tokenizer_decoder.fit_on_texts(train_target_seqs)\n",
    "\n",
    "# Tokenize train\n",
    "# Inputs\n",
    "train_input_data = tokenizer_encoder.texts_to_sequences(train_input_grams)\n",
    "train_input_data = sequence.pad_sequences(train_input_data, maxlen = maxlen_seq, padding = 'post')\n",
    "# Targets\n",
    "train_target_data = tokenizer_decoder.texts_to_sequences(train_target_seqs)\n",
    "train_target_data = sequence.pad_sequences(train_target_data, maxlen = maxlen_seq, padding = 'post')\n",
    "# train_target_data = to_categorical(train_target_data)\n",
    "\n",
    "# Tokenize val\n",
    "# Inputs\n",
    "val_input_data = tokenizer_encoder.texts_to_sequences(val_input_grams)\n",
    "val_input_data = sequence.pad_sequences(val_input_data, maxlen = maxlen_seq, padding = 'post')\n",
    "# Targets\n",
    "val_target_data = tokenizer_decoder.texts_to_sequences(val_target_seqs)\n",
    "val_target_data = sequence.pad_sequences(val_target_data, maxlen = maxlen_seq, padding = 'post')\n",
    "# val_target_data = to_categorical(val_target_data)\n",
    "\n",
    "# Tokenize Test\n",
    "# Inputs\n",
    "test_input_data = tokenizer_encoder.texts_to_sequences(test_input_grams)\n",
    "test_input_data = sequence.pad_sequences(test_input_data, maxlen = maxlen_seq, padding = 'post')\n",
    "# Targets\n",
    "test_target_data = tokenizer_decoder.texts_to_sequences(test_target_seqs)\n",
    "test_target_data = sequence.pad_sequences(test_target_data, maxlen = maxlen_seq, padding = 'post')\n",
    "# test_target_data = to_categorical(test_target_data)\n",
    "\n",
    "\n",
    "'''\n",
    "Resolving dimension issue in target data\n",
    "'''\n",
    "targets = to_categorical(np.r_[train_target_data, val_target_data, test_target_data])\n",
    "train_target_data = targets[:5600]\n",
    "val_target_data = targets[5600: 5872]\n",
    "test_target_data = targets[5872:]\n",
    "\n",
    "print(train_target_data.shape, val_target_data.shape, test_target_data.shape)\n",
    "\n",
    "# Computing the number of words and number of tags to be passed as parameters to the keras model\n",
    "n_words = len(tokenizer_encoder.word_index) + 1\n",
    "n_tags = len(tokenizer_decoder.word_index) + 1\n",
    "\n",
    "print(n_words, n_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "k9HzRWC6kscR",
    "outputId": "df2e3978-4b7e-4c7e-b203-5b96b0d19dc9"
   },
   "outputs": [],
   "source": [
    "train_input_data_alt = train_input_data\n",
    "train_input_data = seq2onehot(train_input_data, n_words)\n",
    "train_profiles = train_df.profiles.values\n",
    "\n",
    "test_input_data_alt = test_input_data\n",
    "test_input_data = seq2onehot(test_input_data, n_words)\n",
    "test_profiles = test_df.profiles.values\n",
    "\n",
    "val_input_data_alt = val_input_data\n",
    "val_input_data = seq2onehot(val_input_data, n_words)\n",
    "val_profiles = val_df.profiles.values\n",
    "\n",
    "print(train_input_data_alt.shape, train_input_data.shape, train_profiles.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "S1uCW8pwk0PC",
    "outputId": "e353e69f-1aac-488a-d0a2-79c73f34a896"
   },
   "outputs": [],
   "source": [
    "train_profiles_np = np.zeros((len(train_profiles), maxlen_seq, 22))\n",
    "for i, profile in enumerate(train_profiles):\n",
    "    for j in range(profile.shape[0]):\n",
    "        for k in range(profile.shape[1]):\n",
    "            train_profiles_np[i, j, k] = profile[j, k]\n",
    "            \n",
    "test_profiles_np = np.zeros((len(test_profiles), maxlen_seq, 22))\n",
    "for i, profile in enumerate(test_profiles):\n",
    "    for j in range(profile.shape[0]):\n",
    "        for k in range(profile.shape[1]):\n",
    "            test_profiles_np[i, j, k] = profile[j, k]\n",
    "            \n",
    "val_profiles_np = np.zeros((len(val_profiles), maxlen_seq, 22))\n",
    "for i, profile in enumerate(val_profiles):\n",
    "    for j in range(profile.shape[0]):\n",
    "        for k in range(profile.shape[1]):\n",
    "            val_profiles_np[i, j, k] = profile[j, k]\n",
    "\n",
    "print(train_profiles_np.shape, test_profiles_np.shape, val_profiles_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "4VdtZrjUltHp",
    "outputId": "d245d6d4-001d-4b7e-d9ae-ce83f4c4a153"
   },
   "outputs": [],
   "source": [
    "num_teams = 4\n",
    "\n",
    "teams_fold_preds_df = pd.read_table('ln2401_kat2193-ddl2133_ks3311-ps2958_jw3468-yw3169_yh3050.csv')\n",
    "teams_fold_preds = teams_fold_preds_df['id,expected']\n",
    "\n",
    "teams_cb6133_preds_df = pd.read_table('cb6133test_ln2401_kat2193-ddl2133_ks3311-ps2958_jw3468-yw3169_yh3050.csv')\n",
    "teams_cb6133_preds = teams_cb6133_preds_df['id,expected']\n",
    "\n",
    "teams_cb513_preds_df = pd.read_table('cb513test_ln2401_kat2193-ddl2133_ks3311-ps2958_jw3468-yw3169_yh3050.csv')\n",
    "teams_cb513_preds = teams_cb513_preds_df['id,expected']\n",
    "\n",
    "train_input_teams = np.zeros((len(teams_fold_preds), 700, num_teams * 9))\n",
    "for i in range(len(teams_fold_preds)):\n",
    "    preds = teams_fold_preds[i].split(',')[1:]\n",
    "    for j in range(700):\n",
    "        if j < len(preds):\n",
    "            for k in range(num_teams):\n",
    "                onehot_idx = k * 9 + q8_list.index(preds[j][k])\n",
    "                train_input_teams[i, j, onehot_idx] = 1\n",
    "        else:\n",
    "            for k in range(num_teams):\n",
    "                onehot_idx = k * 9 + 8\n",
    "                train_input_teams[i, j, onehot_idx] = 1\n",
    "\n",
    "cb6133_input_teams = np.zeros((len(teams_cb6133_preds), 700, num_teams * 9))\n",
    "for i in range(len(teams_cb6133_preds)):\n",
    "    preds = teams_cb6133_preds[i].split(',')[1:]\n",
    "    for j in range(700):\n",
    "        if j < len(preds):\n",
    "            for k in range(num_teams):\n",
    "                onehot_idx = k * 9 + q8_list.index(preds[j][k])\n",
    "                cb6133_input_teams[i, j, onehot_idx] = 1\n",
    "        else:\n",
    "            for k in range(num_teams):\n",
    "                onehot_idx = k * 9 + 8\n",
    "                cb6133_input_teams[i, j, onehot_idx] = 1\n",
    "                \n",
    "print(train_input_teams.shape)\n",
    "print(cb6133_input_teams.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DqeW_BAflXQj"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_input_data, val_input_data, train_target_data, val_target_data\n",
    "X_train_alt, X_val_alt = train_input_data_alt, val_input_data_alt\n",
    "X_train_profile, X_val_profile = train_profiles_np, val_profiles_np\n",
    "X_train_teams, X_val_teams, X_test_teams = train_input_teams, cb6133_input_teams, cb6133_input_teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1581
    },
    "colab_type": "code",
    "id": "jcV01nGalsG9",
    "outputId": "cfd611ec-8394-4b7b-cdd4-70c03dae4c75"
   },
   "outputs": [],
   "source": [
    "def conv_block(x, activation=True, batch_norm=True, drop_out=True, res=True):\n",
    "    c11 = Conv1D(64, 11, padding=\"same\")(x)\n",
    "    \n",
    "    if activation:\n",
    "        c11 = TimeDistributed(Activation(\"relu\"))(c11)\n",
    "    if batch_norm:\n",
    "        c11 = TimeDistributed(BatchNormalization())(c11)\n",
    "    if drop_out:\n",
    "        c11 = TimeDistributed(Dropout(0.5))(c11)\n",
    "    if res:\n",
    "        return Concatenate(axis=-1)([x, c11])\n",
    "    else:\n",
    "        return c11\n",
    "\n",
    "def super_conv_block(x):\n",
    "    c3 = Conv1D(32, 3, padding=\"same\")(x)\n",
    "    c3 = TimeDistributed(Activation(\"relu\"))(c3)\n",
    "    c3 = TimeDistributed(BatchNormalization())(c3)\n",
    "    c3 = TimeDistributed(Dropout(0.5))(c3)\n",
    "    \n",
    "    c7 = Conv1D(64, 7, padding=\"same\")(x)\n",
    "    c7 = TimeDistributed(Activation(\"relu\"))(c7)\n",
    "    c7 = TimeDistributed(BatchNormalization())(c7)\n",
    "    c7 = TimeDistributed(Dropout(0.5))(c7)\n",
    "    \n",
    "    c11 = Conv1D(128, 11, padding=\"same\")(x)\n",
    "    c11 = TimeDistributed(Activation(\"relu\"))(c11)\n",
    "    c11 = TimeDistributed(BatchNormalization())(c11)\n",
    "    c11 = TimeDistributed(Dropout(0.5))(c11)\n",
    "    \n",
    "    x = Concatenate(axis=-1)([x, c3, c7, c11])\n",
    "    return x\n",
    "\n",
    "def CNN_BIGRU():\n",
    "    # inp is one-hot encoded, inp_alt is the same sequence before being one-hot encoded\n",
    "    # We have both since the non-one-hot encoded version is required for Embedding layer\n",
    "    inp = Input(shape = (maxlen_seq, n_words))\n",
    "    inp_alt = Input(shape=(maxlen_seq,))\n",
    "    inp_profiles = Input(shape=(maxlen_seq, 22))\n",
    "    inp_teams = Input(shape=(maxlen_seq, num_teams * n_tags))\n",
    "\n",
    "    #Concatenate embedded and unembedded input\n",
    "    x_emb = Embedding(input_dim = n_words, output_dim = 64, input_length = maxlen_seq)(inp_alt)\n",
    "    x = Concatenate(axis=-1)([inp, x_emb, inp_profiles, inp_teams])\n",
    "\n",
    "    x = super_conv_block(x)\n",
    "    x = conv_block(x)\n",
    "    x = conv_block(x)\n",
    "    x = conv_block(x)\n",
    "\n",
    "    x = Bidirectional(CuDNNGRU(units = 256, return_sequences = True, recurrent_regularizer=l2(0.2)))(x)\n",
    "    x = TimeDistributed(Dense(128, activation = \"relu\"))(x)\n",
    "    x = TimeDistributed(Dense(64, activation = \"relu\"))(x)\n",
    "    y = TimeDistributed(Dense(n_tags, activation = \"softmax\"))(x)\n",
    "\n",
    "    model = Model([inp, inp_alt, inp_profiles, inp_teams], y)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = CNN_BIGRU()\n",
    "\n",
    "model.compile(optimizer = \"Nadam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\", accuracy])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2530
    },
    "colab_type": "code",
    "id": "YGuBMoNho1RP",
    "outputId": "459ed32a-7906-447b-8771-42a731f65ef6"
   },
   "outputs": [],
   "source": [
    "USE_TENSORBOARD = True # Visualize training with Tensorboard\n",
    "if USE_TENSORBOARD:\n",
    "    tbc=TensorBoardColab()\n",
    "    props = dict(verbose=1, callbacks=[TensorBoardColabCallback(tbc)])\n",
    "else:\n",
    "    props = dict(verbose=1)\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train, X_train_alt, X_train_profile, X_train_teams],\n",
    "    y_train,\n",
    "    batch_size = 128,\n",
    "    epochs = 100,\n",
    "    validation_data = ([X_val, X_val_alt, X_val_profile, X_val_teams], y_val),\n",
    "    **props\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "txaDk_SDsHH0"
   },
   "outputs": [],
   "source": [
    "# Print out graph of val acc if history is saved\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "o7NIgwCj1O2R",
    "outputId": "8aa8fb99-9a0f-415b-8cf7-0d06be24a03c"
   },
   "outputs": [],
   "source": [
    "# Save models\n",
    "from google.colab import files\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "files.download('model.json') \n",
    "    \n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "files.download('model.h5')\n",
    "\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fbDdHgSO1Sch",
    "outputId": "bbbb6588-a453-491d-91e8-f40c1a10bf08"
   },
   "outputs": [],
   "source": [
    "revsere_decoder_index = {value:key for key,value in tokenizer_decoder.word_index.items()}\n",
    "revsere_encoder_index = {value:key for key,value in tokenizer_encoder.word_index.items()}\n",
    "\n",
    "y_test_pred = model.predict([test_input_data[:], test_input_data_alt[:], test_profiles_np, X_test_teams])\n",
    "y_test_true = test_target_data[:]\n",
    "\n",
    "y_test_pred\n",
    "\n",
    "print(ex_accuracy(y_test_true, y_test_pred))\n",
    "\n",
    "# for i in range(len(test_input_data)):\n",
    "#     print_results(test_input_seqs[i], y_test_pred[i], revsere_decoder_index)\n",
    "\n",
    "def decode_results(x, y_, revsere_decoder_index):\n",
    "    # print(\"input     : \" + str(x))\n",
    "    # print(\"prediction: \" + str(onehot_to_seq(y_, revsere_decoder_index).upper()))\n",
    "    return str(onehot_to_seq(y_, revsere_decoder_index).upper())\n",
    "\n",
    "decoded_y_pred = []\n",
    "for i in range(len(test_input_data)):\n",
    "    decoded_y_pred.append(decode_results(test_input_seqs[i], y_test_pred[i], revsere_decoder_index))\n",
    "\n",
    "out_df = pd.DataFrame()\n",
    "out_df[\"id\"] = test_df.id.values\n",
    "out_df[\"expected\"] = decoded_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ClFX_8Iu1dWA"
   },
   "outputs": [],
   "source": [
    "with open('cb6133test_stacker_model1_9.csv', 'w') as f:\n",
    "    out_df.to_csv(f, index=False)\n",
    "\n",
    "from google.colab import files\n",
    "files.download('cb6133_stacker2_5.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yPgT9efQ9Tcw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "stacker_on_best.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
